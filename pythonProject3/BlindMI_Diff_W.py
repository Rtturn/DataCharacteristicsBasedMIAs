from BlindMIUtil import *
from dataLoader import *
import tensorflow as tf
from tensorflow.keras.models import load_model
import sys
import pandas as pd
import scipy.io as io

os.environ['CUDA_VISIBLE_DEVICES'] = '0'
tf.config.experimental.set_memory_growth(tf.config.experimental.list_physical_devices('GPU')[0], True)
DATA_NAME = sys.argv[1] if len(sys.argv) > 1 else "CIFAR"
TARGET_MODEL_GENRE = sys.argv[2] if len(sys.argv) > 2 else "ResNet50"
TARGET_WEIGHTS_PATH = "weights/Target/{}_{}.hdf5".format(DATA_NAME, TARGET_MODEL_GENRE)

(x_train_tar, y_train_tar), (x_test_tar, y_test_tar), m_true = globals()['load_' + DATA_NAME]('TargetModel')
Target_Model = load_model(TARGET_WEIGHTS_PATH)

x_train_tar=x_train_tar[:420]
y_train_tar=y_train_tar[:420]
labels=pd.read_excel('incorrect.xlsx')
labels=np.array(labels)[:,1]
labels=labels[:420]
x_tt=[]
y_tt=[]
for i in labels:
    x_tt.append(x_test_tar[i])
    y_tt.append(y_test_tar[i])
x_test_tar=np.array(x_tt)
y_test_tar=np.array(y_tt)
m_true=m_true[:420]
m_true=np.r_[m_true,np.zeros(420)]

def diff_Mem_attack(x_, y_true, m_true, target_model, non_Mem_Generator=sobel):
    '''
    Attck the target with BLINDMI-DIFF-W, BLINDMI-DIFF with gernerated non-member.
    The non-member is generated by randomly chosen data and the number is 20 by default.
    If the data has been shuffled, please directly remove the process of shuffling.
    :param target_model: the model that will be attacked
    :param x_: the data that target model may used for trainisng
    :param y_true: the label of x_
    :param m_true: one of 0 and 1, which represents each of x_ has been trained or not.
    :param non_Mem_Generator: the method to generate the non-member data. The default non-member generator
    is Sobel.
    :return:  Tensor arrays of results
    '''
    y_pred=target_model.predict(x_)
    m,n=np.shape(y_pred)
    y_result=np.zeros((m,n))
    correct_index=[]
    correct_label=[]
    correct_pred=[]
    incorrect_index=[]
    incorrect_label=[]
    incorrect_pred=[]
    for i in range(m):
        interval=list(y_pred[i])
        index=interval.index(max(interval))
        y_result[i][index]=1
        true=list(y_true[i])
        true_index=true.index(max(true))
        if index==true_index:
            correct_index.append(i)
            correct_label.append(y_true[i])
            correct_pred.append(y_pred[i])
        else:
            incorrect_index.append(i)
            incorrect_label.append(y_true[i])
            incorrect_pred.append(y_pred[i])
    correct_pred=tf.convert_to_tensor(correct_pred)
    incorrect_pred=tf.convert_to_tensor(incorrect_pred)
    correct=tf.convert_to_tensor(np.c_[correct_pred[y_true[correct_index].astype(bool)],
                                        np.sort(correct_pred, axis=1)[:, ::-1][:, :2]])
    incorrect=tf.convert_to_tensor(np.c_[incorrect_pred[y_true[incorrect_index].astype(bool)],
                                        np.sort(incorrect_pred, axis=1)[:, ::-1][:, :2]])
#    incorrect_read=pd.DataFrame(incorrect_index)
#    incorrect_read.to_excel('incorrect.xlsx')
    mix = np.c_[y_pred[y_true.astype(bool)], np.sort(y_pred, axis=1)[:, ::-1][:, :2]]  #所有数据变换后的结果
    mix=mix.tolist()
    for i in range(m):
        if i in correct_index:
            mix[i].append(1)
        else:
            mix[i].append(0)
    mix=tf.convert_to_tensor(mix)
    nonMem_index = np.random.randint(0, x_.shape[0], size=20)  #随机生成
    print(nonMem_index)
    Mem_dataset=[]
    Mem_index=[]
    for i in range(m):
        if m_true[i]==1:
            Mem_dataset.append(x_[i])
            Mem_index.append(i)
    Mem_dataset=tf.convert_to_tensor(Mem_dataset)
    Mem_pred=target_model.predict(Mem_dataset)
    Mem=tf.convert_to_tensor(np.c_[Mem_pred[y_true[Mem_index].astype(bool)],
                                        np.sort(Mem_pred, axis=1)[:, ::-1][:, :2]])
    nonMem_pred = target_model.predict(non_Mem_Generator(x_[nonMem_index]))
    nonMem = tf.convert_to_tensor(np.c_[nonMem_pred[y_true[nonMem_index].astype(bool)],
                                        np.sort(nonMem_pred, axis=1)[:, ::-1][:, :2]])
    print(mmd_loss(nonMem,correct[:20][:],weight=1))
    print(mmd_loss(nonMem,incorrect[:20][:],weight=1))
    print(mmd_loss(nonMem,Mem[20:40][:],weight=1))
    print(mmd_loss(Mem[:20][:],incorrect[:20][:],weight=1))
    print(mmd_loss(Mem[:20][:],correct[20:40][:],weight=1))

    data = tf.data.Dataset.from_tensor_slices((mix, m_true)).shuffle(buffer_size=x_.shape[0]).\
        batch(20).prefetch(tf.data.experimental.AUTOTUNE)  #切分mix为20000个10列的tensor，每个数据集有20个样本

    m_pred, m_true = [], []
    mix_shuffled = []
    count_correct=[]
    count_incorrect=[]
    count_member=[]
    count_nonmember=[]
    count_member_correct=[]
    count_member_incorrect=[]
    count_nonmember_correct=[]
    count_nonmember_incorrect=[]
    precision=[]
    distance_nonc_mem=[]
    distance_nonc_none=[]
    distance_none_mem=[]
    distance_non_mem=[]
    nonmem_in_result=[]
    nonmem_correct_in_result=[]
    distance_iter=[]
    distance_corr=[]
    distance_mem=[]
    distance_avg_corr=[]
    distance_avg_mem=[]
    m_pred_member=[]
    iter_number=0
    similar_of_dataset=[]

    for (mix_batch, m_true_batch) in data:  #min_batch:x  m_true_batch:是否为成员
        iter_number+=1
        m_pred_batch = np.ones(mix_batch.shape[0])  #1000*1
        m_pred_epoch = np.ones(mix_batch.shape[0])  #1000*1
        nonMemInMix = True
        countc=0
        counte=0
        countmc=0
        countnmc=0
        countme=0
        countnme=0
        countm=0
        countnm=0
        dis_nonc_mem=9999
        dis_nonc_none=9999
        dis_none_mem=9999
        dis_non_mem=9999
        nonmem_correct_dataset = []
        mem_correct_dataset=[]
        nonmem_incorrect_dataset=[]
        nonmem_dataset=[]
        correct_index_epoch = [0] * 20
        for i in range(20):
            if mix_batch[i][3]==1:
                countc+=1
                correct_index_epoch[i]=1
                if m_true_batch[i]==1:
                    countmc+=1
                    countm+=1
                    mem_correct_dataset.append(mix_batch[i,:3])
                else:
                    countnmc+=1
                    countnm+=1
                    nonmem_correct_dataset.append(mix_batch[i,:3])
                    nonmem_dataset.append(mix_batch[i,:3])
            else:
                counte+=1
                if m_true_batch[i]==1:
                    countme+=1
                    countm+=1
                else:
                    countnme+=1
                    countnm+=1
                    nonmem_incorrect_dataset.append(mix_batch[i,:3])
                    nonmem_dataset.append(mix_batch[i,:3])
        mem_correct_dataset=tf.convert_to_tensor(np.mat(mem_correct_dataset))
        nonmem_dataset=tf.convert_to_tensor(np.mat(nonmem_dataset))
        nonmem_correct_dataset=tf.convert_to_tensor(np.mat(nonmem_correct_dataset))
        nonmem_incorrect_dataset=tf.convert_to_tensor(np.mat(nonmem_incorrect_dataset))
        if len(nonmem_correct_dataset[0])>0 and len(nonmem_correct_dataset)>0 and len(nonmem_incorrect_dataset[0])>0 and len(nonmem_incorrect_dataset)>0 and len(mem_correct_dataset[0])>0 and len(mem_correct_dataset)>0:
            dis_nonc_mem=mmd_loss(nonmem_correct_dataset,mem_correct_dataset,weight=1)
            dis_nonc_none=mmd_loss(nonmem_incorrect_dataset,nonmem_correct_dataset,weight=1)
            dis_none_mem=mmd_loss(nonmem_incorrect_dataset,mem_correct_dataset,weight=1)
            dis_non_mem=mmd_loss(nonmem_dataset,mem_correct_dataset,weight=1)
        distance_none_mem.append(dis_none_mem)
        distance_nonc_mem.append(dis_nonc_mem)
        distance_nonc_none.append(dis_nonc_none)
        distance_non_mem.append(dis_non_mem)
        count_correct.append(countc)
        count_incorrect.append(counte)
        count_member_correct.append(countmc)
        count_member_incorrect.append(countme)
        count_nonmember_correct.append(countnmc)
        count_nonmember_incorrect.append(countnme)
        count_member.append(countm)
        count_nonmember.append(countnm)
        precision.append(float(countc/20))
        mix_batchs=mix_batch[:,:3]

        dis_iter=mmd_loss(nonMem, mix_batchs, weight=1)
        distance_iter.append(dis_iter)

        index_correct=[]
        index_member=[]
        for i in range(20):
            if m_true_batch[i]==1:
                index_member.append(i)
            if correct_index_epoch[i]==1 and m_true_batch[i]==0:
                index_correct.append(i)

        sum_corr=0
        sum_mem=0
        for i in index_correct:
            nonmem_batch_correct=tf.concat([nonMem,[mix_batchs[i]]],axis=0)
            mix_batch_correct=tf.concat([mix_batchs[:i],mix_batchs[i+1:]],axis=0)
            m_pred_correct=np.r_[m_pred_batch[:i],m_pred_batch[i+1:]]
            mix_batch_correct=mix_batch_correct[m_pred_correct.astype(bool,copy=True)]
            dis_move_corr=mmd_loss(nonmem_batch_correct,mix_batch_correct,weight=1)
            distance_corr.append([iter_number,dis_move_corr])
            sum_corr+=dis_move_corr
        if len(index_correct)!=0:
            distance_avg_corr.append(sum_corr/len(index_correct))
        else:
            distance_avg_corr.append(9999)

        for i in index_member:
            nonmem_batch_mem=tf.concat([nonMem,[mix_batchs[i]]],axis=0)
            mix_batch_mem=tf.concat([mix_batchs[:i],mix_batchs[i+1:]],axis=0)
            m_pred_mem=np.r_[m_pred_batch[:i],m_pred_batch[i+1:]]
            mix_batch_mem=mix_batch_mem[m_pred_mem.astype(bool,copy=True)]
            dis_move_mem=mmd_loss(nonmem_batch_mem,mix_batch_mem,weight=1)
            distance_mem.append([iter_number,dis_move_mem])
            sum_mem+=(dis_move_mem)
        distance_avg_mem.append(sum_mem/len(index_member))

        while nonMemInMix:  #迭代
            mix_epoch_new = mix_batchs[m_pred_epoch.astype(bool)]
            dis_ori = mmd_loss(nonMem, mix_epoch_new, weight=1)
            nonMemInMix = False  #flag
            for index, item in tqdm(enumerate(mix_batchs)): #对于每个x
                if m_pred_batch[index] == 1:  #如果没有被选择过
                    nonMem_batch_new = tf.concat([nonMem, [mix_batchs[index]]], axis=0)  #将其加入nonmember
                    mix_batch_new = tf.concat([mix_batchs[:index], mix_batchs[index+1:]], axis=0)  #将其摘出mixbatch
                    m_pred_without = np.r_[m_pred_batch[:index], m_pred_batch[index+1:]]  #不用它进行预测
                    mix_batch_new = mix_batch_new[m_pred_without.astype(bool, copy=True)]  #更新target的概率分布
                    dis_new = mmd_loss(nonMem_batch_new, mix_batch_new, weight=1)
                    if dis_new > dis_ori:
                        nonMemInMix = True  #如果有改变，继续迭代
                        m_pred_epoch[index] = 0  #设为非成员
            dis_iter=dis_ori
            m_pred_batch = m_pred_epoch.copy()

        mix_shuffled.append(mix_batchs)
        m_pred.append(m_pred_batch)
        m_true.append(m_true_batch)
        count_nomem_to_mem=0
        count_nomem_correct_to_mem=0
        for i in range(20):
            if m_pred_batch[i]==1 and m_true_batch[i]==0:
                count_nomem_to_mem+=1
            if m_pred_batch[i] == 1 and m_true_batch[i] == 0 and correct_index_epoch[i]==1:
                count_nomem_correct_to_mem+=1
        nonmem_in_result.append(count_nomem_to_mem)
        nonmem_correct_in_result.append(count_nomem_correct_to_mem)
        m_pred_member.append(sum(m_pred_batch))

    result=np.zeros((len(count_correct),19))
    for i in range(len(count_correct)):
        result[i,0]=count_correct[i]
        result[i,1]=count_incorrect[i]
        result[i,2]=count_member[i]
        result[i,3]=count_nonmember[i]
        result[i,4]=count_member_correct[i]
        result[i,5]=count_member_incorrect[i]
        result[i,6]=count_nonmember_correct[i]
        result[i,7]=count_nonmember_incorrect[i]
        result[i,8]=distance_none_mem[i]
        result[i,9]=distance_non_mem[i]
        result[i,10]=precision[i]
        result[i,11]=distance_nonc_mem[i]
        result[i,12]=distance_nonc_none[i]
        result[i,13]=nonmem_in_result[i]
        result[i,14]=nonmem_correct_in_result[i]
        result[i,15]=m_pred_member[i]
        result[i,16]=distance_iter[i]
        result[i,17]=distance_avg_corr[i]
        result[i,18]=distance_avg_mem[i]
    result=np.array(result)
    result=pd.DataFrame(result)
    writer=pd.ExcelWriter('result_of_constrcut.xlsx')
    result.to_excel(writer,'result')
    writer.save()
    writer.close()

    result1 = np.array(distance_corr)
    result1=pd.DataFrame(result1)
    result2=np.array(distance_mem)
    result2=pd.DataFrame(result2)
    writer = pd.ExcelWriter('A.xlsx')  # 写入Excel文件
    result1.to_excel(writer, "page_1")
    result2.to_excel(writer,'page_2')
    writer.save()
    writer.close()

    return np.concatenate(m_true, axis=0), np.concatenate(m_pred, axis=0), \
           np.concatenate(mix_shuffled, axis=0), nonMem

m_true, m_pred, mix, nonMem = diff_Mem_attack(np.r_[x_train_tar,x_test_tar],np.r_[y_train_tar,y_test_tar],m_true,Target_Model)
evaluate_attack(m_true, m_pred)